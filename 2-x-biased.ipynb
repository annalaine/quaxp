{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Information\n",
    "__Section__: Example of biased image data  \n",
    "__Goal__: Understand how data can introduce bias to a project.  \n",
    "__Time needed__: x min  \n",
    "__Prerequisites__: None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we introduce an example of a situation when biased image data had an impact on the result of a machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biased beauty contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2016, an international beauty contest took place online: [Beauty.AI](http://winners2.beauty.ai/) had the particularity to let machine learning algorithms decide on the winners. The idea was to get free of any human bias in the perception of beauty, the project being used later for gauging health from a picture of the face.\n",
    "\n",
    "This is a collection of the 44 winners of this international beauty contest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](2-x-beautyai.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, despite the contest being international, a vast majority of the winners is light-skinned. Not so much of a judgment \"free of bias\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on how this has happened. Here is a simplified judge model to create a beauty score based on photos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](2-x-beautyai-model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes as an input the photo of the participant, and outputs a score.\n",
    "\n",
    "1. The model has to analyze the photo with image processing methods.\n",
    "2. The result of the image analysis is a set of features previously manually determined by the data scientist.\n",
    "3. The computed features are then passed into a regression model, which can compute a score.\n",
    "\n",
    "With the scores of all participants, the overall judge model can select the winner, having the highest score.\n",
    "\n",
    "In this example, we can believe that a bias happens in the first step of the model: the image processing. The computation of the features is different if we speak about a light-skinned face and a dark-skinned one: light, contrast and shadows are different.\n",
    "\n",
    "To fix this issue, one can think of adding a step when computing the features: first, the skin color is determined, and then the model can choose an appropriate method to calculate the features.\n",
    "\n",
    "Is it important to note that if the dataset used to create the model in the first place (the training dataset) was containing the same amount of dark and light-skinned faces, the result would probably not have been so easily biased, as the model would have learned equally from dark-skinned faces and light-skinned ones. This kind of bias is called __sampling bias__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](2-x-sampling-bias.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this picture, we represented the concept of sampling bias: it happens when the training environment does not match the environment in which the model will be used. In this case, the training data contains most circle shapes and one square shape: the model will then learn a pattern on the circle shapes, and consider the square shape a noise. If the running environment contains more square shapes, the model is not able to make good predicitions on them, as it didn't have enough data to learn a pattern with them.\n",
    "\n",
    "We can make the paralell with our example: if the training dataset doesn't have enough dark-skinned faces compared to light-skinned ones, it is not able to learn how to correctly process them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tip:__ note that this kind of bias is not exclusive for image data, and can happen with any kind of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
